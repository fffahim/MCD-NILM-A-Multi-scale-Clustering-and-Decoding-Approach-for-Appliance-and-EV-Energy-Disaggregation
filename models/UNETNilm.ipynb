{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "   \n",
    "    def __init__(self, \n",
    "                 n_channels, \n",
    "                 n_kernels,\n",
    "                 kernel_size=3, \n",
    "                 stride=2, \n",
    "                 padding=1, \n",
    "                 last=False, \n",
    "                 activation=nn.PReLU()):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            n_channels, n_kernels,\n",
    "            kernel_size, stride, padding\n",
    "        )\n",
    "        if not last:\n",
    "            self.net = nn.Sequential(\n",
    "                self.conv,\n",
    "                nn.BatchNorm1d(n_kernels),\n",
    "                activation)\n",
    "        else:\n",
    "            self.net = self.conv\n",
    "        nn.utils.weight_norm(self.conv)    \n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  \n",
    "        \n",
    "class Deconv1D(nn.Module):\n",
    "   \n",
    "    def __init__(self, \n",
    "                 n_channels, \n",
    "                 n_kernels,\n",
    "                 kernel_size=3, \n",
    "                 stride=2, \n",
    "                 padding=1, \n",
    "                 last=False, \n",
    "                 activation=nn.PReLU()):\n",
    "        super(Deconv1D, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(\n",
    "            n_channels, n_kernels,\n",
    "            kernel_size, stride, padding\n",
    "        )\n",
    "        if not last:\n",
    "            self.net = nn.Sequential(\n",
    "                self.deconv,\n",
    "                nn.BatchNorm1d(n_kernels),\n",
    "                activation\n",
    "            )\n",
    "        else:\n",
    "            self.net = self.deconv\n",
    "        nn.init.xavier_uniform_(self.deconv.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)          \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_channels=10, \n",
    "                 n_kernels=16, \n",
    "                 n_layers=3, \n",
    "                 seq_size=50):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.feat_size = (seq_size-1) // 2**n_layers +1\n",
    "        self.feat_dim = self.feat_size * n_kernels\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            *([Conv1D(n_channels, n_kernels // 2**(n_layers-1))] +\n",
    "              [Conv1D(n_kernels//2**(n_layers-l),\n",
    "                         n_kernels//2**(n_layers-l-1))\n",
    "               for l in range(1, n_layers-1)] +\n",
    "              [Conv1D(n_kernels // 2, n_kernels, last=True)])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        assert len(x.size())==3\n",
    "        feats = self.conv_stack(x)\n",
    "        return feats\n",
    "        \n",
    "\n",
    "class Up(nn.Module):\n",
    "   \n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.upsample = Deconv1D(in_ch, in_ch // 2)\n",
    "        self.conv = Conv1D(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        # Pad x1 to the size of x2\n",
    "        diff = x2.shape[2] - x1.shape[2]\n",
    "        x1 = F.pad(x1, [diff// 2, diff - diff // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetCNN1D(nn.Module):\n",
    "   \n",
    "    def __init__(\n",
    "            self, \n",
    "            num_layers: int = 5,\n",
    "            features_start: int = 8 * 8,\n",
    "            n_channels: int =1,\n",
    "            num_classes: int = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        layers = [Conv1D(n_channels, features_start)]\n",
    "        feats = features_start\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.append(Conv1D(feats, feats * 2))\n",
    "            feats *= 2\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Up(feats, feats // 2))\n",
    "            feats //= 2\n",
    "        \n",
    "       \n",
    "        conv = nn.Conv1d(feats, num_classes, kernel_size=1)\n",
    "        conv = nn.utils.weight_norm(conv)\n",
    "        nn.init.xavier_uniform_(conv.weight)\n",
    "        layers.append(conv)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xi = [self.layers[0](x)]\n",
    "        \n",
    "        for layer in self.layers[1:self.num_layers]:\n",
    "            xi.append(layer(xi[-1]))\n",
    "            \n",
    "        for i, layer in enumerate(self.layers[self.num_layers:-1]):\n",
    "            xi[-1] = layer(xi[-1], xi[-2 - i])\n",
    "            \n",
    "        out = self.layers[-1](xi[-1])\n",
    "        return out\n",
    "    \n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, in_size, \n",
    "                 hidden_arch=[128], \n",
    "                 output_size=None, \n",
    "                 activation=nn.PReLU(),\n",
    "                 batch_norm=True):\n",
    "        \n",
    "        super(MLPLayer, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.output_size = output_size\n",
    "        layer_sizes = [in_size] + [x for x in hidden_arch]\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            self.layers.append(layer)\n",
    "                    \n",
    "            if batch_norm and i!=0:\n",
    "                bn = nn.BatchNorm1d(layer_sizes[i+1])\n",
    "                self.layers.append(bn)\n",
    "     \n",
    "            self.layers.append(activation)\n",
    "           \n",
    "        if output_size is not None:\n",
    "            layer = nn.Linear(layer_sizes[-1], output_size)\n",
    "            self.layers.append(layer)\n",
    "            self.layers.append(activation)\n",
    "            \n",
    "        self.init_weights()\n",
    "        self.mlp_network =  nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.mlp_network(z)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for layer in self.layers:\n",
    "            try:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.utils.weight_norm(layer)\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "            except: pass\n",
    "\n",
    "\n",
    "class UNETNilm(nn.Module):\n",
    "    def __init__(self, in_size=1, \n",
    "                 output_size=5,\n",
    "                 d_model=128, \n",
    "                 dropout=0.1, \n",
    "                 seq_len=99, \n",
    "                 features_start=16,  \n",
    "                 n_layers=8, \n",
    "                 n_quantiles=5, \n",
    "                 pool_filter=8):\n",
    "        super().__init__()\n",
    "        self.unet = UNetCNN1D(num_classes=output_size, num_layers=n_layers, features_start=features_start, n_channels=in_size)\n",
    "        self.conv_layer = Encoder(n_channels=output_size, n_kernels=d_model, n_layers=n_layers//2, seq_size=seq_len)\n",
    "        # self.mlp_layer = nn.Linear(d_model*pool_filter, 128)\n",
    "        self.mlp_layer = MLPLayer(in_size=d_model*pool_filter)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pool_filter = pool_filter\n",
    "        self.n_quantiles = n_quantiles\n",
    "        \n",
    "        # self.fc_out_state  = nn.Linear(256, output_size)\n",
    "        self.fc_out_power  = nn.Linear(128, output_size)\n",
    "        # nn.utils.weight_norm(self.mlp_layer)\n",
    "        # nn.init.xavier_normal_(self.mlp_layer.weight)\n",
    "\n",
    "        # nn.utils.weight_norm(self.fc_out_power)\n",
    "        nn.init.xavier_normal_(self.fc_out_power.weight)\n",
    "\n",
    "        # nn.utils.weight_norm(self.fc_out_state)\n",
    "        # nn.init.xavier_normal_(self.fc_out_state.weight)\n",
    "\n",
    "        # self.fc_out_state.bias.data.fill_(0)\n",
    "        self.fc_out_power.bias.data.fill_(0)\n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = x.permute(0,2,1)\n",
    "        unet_out = self.dropout(self.unet(x))\n",
    "        conv_out = self.conv_layer(unet_out)\n",
    "        conv_out = self.dropout(F.adaptive_avg_pool1d(conv_out, self.pool_filter).reshape(x.size(0), -1))\n",
    "        mlp_out  = self.mlp_layer(conv_out)\n",
    "        # states_logits   = self.fc_out_state(mlp_out).reshape(B, 1, -1)\n",
    "        power_logits    = self.fc_out_power(mlp_out).reshape(B, -1)\n",
    "        # if self.n_quantiles>1:\n",
    "        #     power_logits = power_logits.reshape(B, self.n_quantiles, -1)\n",
    "       \n",
    "        return  power_logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
