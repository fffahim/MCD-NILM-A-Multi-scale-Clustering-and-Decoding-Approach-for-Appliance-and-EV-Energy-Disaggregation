{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3bd8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImprovedGMMClusterAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=64, num_clusters=3, use_positional_encoding=True):\n",
    "        super().__init__()\n",
    "        self.K = num_clusters\n",
    "        self.D = input_dim\n",
    "        self.E = embed_dim\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "\n",
    "        # Temporal projection to embedding space\n",
    "        self.temporal_conv = nn.Conv1d(input_dim, embed_dim, kernel_size=5, padding=2)\n",
    "        self.batch_norm = nn.BatchNorm1d(embed_dim, momentum=0.9)\n",
    "        self.project = nn.Linear(embed_dim, embed_dim)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.instance_norm = nn.InstanceNorm1d(embed_dim, affine=True, momentum=0.9)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # GMM parameters in embedding space\n",
    "        self.means = nn.Parameter(torch.randn(self.K, embed_dim))            # [K, E]\n",
    "        self.log_vars = nn.Parameter(torch.zeros(self.K, embed_dim))         # [K, E]\n",
    "        self.logits_pi = nn.Parameter(torch.zeros(self.K))                   # [K]\n",
    "        self.temperature = nn.Parameter(torch.tensor(1.0))                   # Learnable\n",
    "\n",
    "    def get_positional_encoding(self, seq_len, dim, device):\n",
    "        pe = torch.zeros(seq_len, dim, device=device)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2, device=device).float() * (-math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # [1, T, D]\n",
    "\n",
    "    def gmm_prob(self, x_embed):  # [B, T, E]\n",
    "        B, T, E = x_embed.shape\n",
    "        x = x_embed.unsqueeze(2)                        # [B, T, 1, E]\n",
    "        mu = self.means.unsqueeze(0).unsqueeze(0)       # [1, 1, K, E]\n",
    "        log_var = self.log_vars.unsqueeze(0).unsqueeze(0)  # [1, 1, K, E]\n",
    "        var = torch.exp(log_var)                        # [1, 1, K, E]\n",
    "\n",
    "        # Log Gaussian probability\n",
    "        log_prob = -0.5 * ((x - mu)**2 / var + log_var + math.log(2 * math.pi))  # [B, T, K, E]\n",
    "        log_prob = log_prob.sum(dim=-1)  # [B, T, K]\n",
    "\n",
    "        # Add log priors\n",
    "        log_pi = F.log_softmax(self.logits_pi, dim=-1)  # [K]\n",
    "        log_prob = log_prob + log_pi                    # [B, T, K]\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        log_prob = log_prob / torch.clamp(self.temperature, min=0.1, max=10.0)\n",
    "\n",
    "        probs = F.softmax(log_prob, dim=-1)  # [B, T, K]\n",
    "        return probs, log_prob\n",
    "\n",
    "    def forward(self, features):  # [B, T, D]\n",
    "        B, T, D = features.shape\n",
    "\n",
    "        # Optional positional encoding\n",
    "        # if self.use_positional_encoding:\n",
    "        #     pe = self.get_positional_encoding(T, D, features.device)\n",
    "        #     features = features + pe\n",
    "\n",
    "        # Temporal embedding\n",
    "        x = self.batch_norm(self.temporal_conv(features.transpose(1, 2))).transpose(1, 2)  # [B, T, E]\n",
    "        x_embed = self.ln(self.project(x))  # [B, T, E]\n",
    "\n",
    "        # GMM soft assignment\n",
    "        weights, log_probs = self.gmm_prob(x_embed)  # [B, T, K], [B, T, K]\n",
    "\n",
    "        # Feature aggregation\n",
    "        cluster_features = torch.einsum('btk,btd->bktd', weights, features)  # [B, K, T, D]\n",
    "\n",
    "        return cluster_features, weights, log_probs\n",
    "    \n",
    "class FuzzyCMeansClustering(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=64, num_clusters=3, fuzziness=2.0, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.fuzziness = fuzziness\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Temporal embedding\n",
    "        self.temporal_conv = nn.Conv1d(input_dim, embed_dim, kernel_size=5, padding=2)\n",
    "        self.batch_norm = nn.BatchNorm1d(embed_dim, momentum=0.9)\n",
    "        self.project = nn.Linear(embed_dim, embed_dim)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Cluster centers in embedded space\n",
    "        self.cluster_centers = nn.Parameter(torch.randn(num_clusters, embed_dim))\n",
    "        nn.init.xavier_uniform_(self.cluster_centers)\n",
    "\n",
    "        # Learnable temperature\n",
    "        self.temperature = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, features):  # features: [B, T, D]\n",
    "        B, T, D = features.shape\n",
    "\n",
    "        # Temporal embedding\n",
    "        x = self.temporal_conv(features.transpose(1, 2))      # [B, E, T]\n",
    "        x = self.batch_norm(x).transpose(1, 2)                 # [B, T, E]\n",
    "        x_embed = self.ln(self.project(x))                     # [B, T, E]\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        x_norm = F.normalize(x_embed, p=2, dim=-1)             # [B, T, E]\n",
    "        centers = F.normalize(self.cluster_centers, p=2, dim=-1)  # [K, E]\n",
    "\n",
    "        # Cosine-based fuzzy distance\n",
    "        cos_sim = torch.matmul(x_norm, centers.T)              # [B, T, K]\n",
    "        dist_sq = 2 - 2 * cos_sim                              # [B, T, K]\n",
    "        dist_sq = torch.clamp(dist_sq, min=self.epsilon)\n",
    "\n",
    "        # Fuzzy membership weights\n",
    "        power = 1.0 / (self.fuzziness - 1)\n",
    "        inv_dist = torch.pow(dist_sq, -power)                  # [B, T, K]\n",
    "        weights = inv_dist / (inv_dist.sum(dim=-1, keepdim=True) + self.epsilon)\n",
    "\n",
    "        # Temperature scaling (optional)\n",
    "        weights = weights / torch.clamp(self.temperature, min=0.1, max=10.0)\n",
    "\n",
    "        # Weighted feature aggregation using **original features**\n",
    "        cluster_features = torch.einsum('btk,btd->bktd', weights, features)  # [B, K, T, D]\n",
    "\n",
    "        return cluster_features, weights\n",
    "\n",
    "    \n",
    "class IBNNet(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=5, stride=1, padding='same')\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim, momentum=0.9)\n",
    "        self.lr1 = nn.LayerNorm(hidden_dim)  # Layer normalization after conv1\n",
    "\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=1, stride=1, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim, momentum=0.9)\n",
    "        self.lr2 = nn.LayerNorm(hidden_dim)  # Layer normalization after conv2\n",
    "\n",
    "        # self.conv_middle = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1, stride=1, padding='same')\n",
    "        # self.bn_middle = nn.BatchNorm1d(hidden_dim, momentum=0.9)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(hidden_dim, 256, kernel_size=5, stride=1, padding='same')\n",
    "        self.bn3 = nn.BatchNorm1d(256, momentum=0.9)\n",
    "\n",
    "        self.lr3 = nn.LayerNorm(256)  # Layer normalization after conv3\n",
    "        self.instance_norm = nn.InstanceNorm1d(256, affine=True, momentum=0.9)\n",
    "\n",
    "    def forward(self, x, res=True, instance_norm=True):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # [B, hidden_dim, T]\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # [B, hidden_dim, T]\n",
    "        # x = F.relu(self.bn_middle(self.conv_middle(x)))  # [B, hidden_dim, T]\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        if res == True:\n",
    "            x = x + residual\n",
    "\n",
    "        if instance_norm:\n",
    "            x = self.instance_norm(x)\n",
    "        else:\n",
    "            x = self.bn3(x)\n",
    "        # x = x.transpose(1, 2)  # [B, T, D]\n",
    "        # x = self.lr(x)  # Apply layer normalization\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoderhelper1 = IBNNet(input_dim, hidden_dim)\n",
    "        self.upconv1 = nn.ConvTranspose1d(input_dim * 2, input_dim, kernel_size=kernel_size, stride=1, padding=padding)  # Upsample x2\n",
    "        self.bn1 = nn.BatchNorm1d(input_dim, momentum=0.9)\n",
    "        # self.conv1 = nn.Conv1d(input_dim * 2, input_dim, kernel_size=3, padding=1)\n",
    "        # self.bn2 = nn.BatchNorm1d(input_dim, momentum=0.9)\n",
    "\n",
    "        self.encoderhelper2 = IBNNet(input_dim, hidden_dim)\n",
    "        self.upconv2 = nn.ConvTranspose1d(input_dim * 2, input_dim, kernel_size=kernel_size, stride=1, padding=padding)  # Upsample x2\n",
    "        self.bn3 = nn.BatchNorm1d(input_dim, momentum=0.9)\n",
    "\n",
    "        self.encoderhelper3 = IBNNet(input_dim, hidden_dim)\n",
    "        self.upconv3 = nn.ConvTranspose1d(input_dim * 2, input_dim, kernel_size=kernel_size, stride=1, padding=padding)  # Upsample x2\n",
    "        self.bn4 = nn.BatchNorm1d(input_dim, momentum=0.9)\n",
    "\n",
    "        self.encoderhelper4 = IBNNet(input_dim, hidden_dim)\n",
    "        self.upconv4 = nn.ConvTranspose1d(input_dim * 2, input_dim, kernel_size=kernel_size, stride=1, padding=padding)  # Upsample x2\n",
    "        self.bn5 = nn.BatchNorm1d(input_dim, momentum=0.9)\n",
    "\n",
    "        self.encoderhelper5 = IBNNet(input_dim, hidden_dim)\n",
    "        self.upconv5 = nn.ConvTranspose1d(input_dim * 2, input_dim // 2, kernel_size=kernel_size, stride=1, padding=padding)  # Upsample x2\n",
    "        self.bn6 = nn.BatchNorm1d(input_dim // 2, momentum=0.9)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(input_dim, input_dim // 2, kernel_size=kernel_size, padding=padding)\n",
    "        self.bn6 = nn.BatchNorm1d(input_dim // 2, momentum=0.9)\n",
    "\n",
    "        # self.fc = nn.Linear(input_dim // 2, output_dim)\n",
    "\n",
    "        self.fc_list = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                # nn.Conv1d(input_dim, 1, kernel_size=5, padding=2),\n",
    "                # nn.BatchNorm1d(1, momentum=0.9),\n",
    "                # Transpose(1, 2)\n",
    "                nn.Linear(input_dim * 2, input_dim),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(input_dim, 1)\n",
    "            ) for _ in range(output_dim)  # One for each cluster\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, enc2, enc1):\n",
    "\n",
    "        x = self.encoderhelper1(x, res=False)  # x: [B, C, T]\n",
    "        # x: [B, C, T], enc2 and enc1 have same dims for concat\n",
    "        x = torch.cat([x, enc2], dim=1)                  # Concat skip connection\n",
    "        x = F.relu(self.bn1(self.upconv1(x)))            # Upsample\n",
    "        # x = F.relu(self.bn2(self.conv1(x)))\n",
    "\n",
    "        x = self.encoderhelper2(x, res=True, instance_norm=False)  # x: [B, C, T]\n",
    "        x = torch.cat([x, enc1], dim=1)                  # Concat skip connection\n",
    "        # x = F.relu(self.bn3(self.upconv2(x)))            # Upsample\n",
    "\n",
    "        # x = self.encoderhelper3(x, res=True, instance_norm=False)  # x: [B, C, T]\n",
    "        # x = torch.cat([x, enc1], dim=1)                  # Concat skip connection\n",
    "        # x = F.relu(self.bn4(self.upconv3(x)))            # Upsample\n",
    "\n",
    "        # x = self.encoderhelper4(x, res=True, instance_norm=False)  # x: [B, C, T]\n",
    "        # x = torch.cat([x, enc1], dim=1)                  # Concat skip connection\n",
    "        # x = F.relu(self.bn5(self.upconv4(x)))            # Upsample\n",
    "\n",
    "        # x = self.encoderhelper5(x, res=True)  # x: [B, C, T]\n",
    "        # x = torch.cat([x, enc1], dim=1)                  # Concat\n",
    "\n",
    "        # x = F.relu(self.bn6(self.conv2(x)))\n",
    "\n",
    "        # Global average pool over time dimension\n",
    "        # x = torch.mean(x, dim=2)  # [B, C]\n",
    "        x = x.transpose(1, 2)  # [B, T, C] for linear layers\n",
    "        outputs = []\n",
    "        for fc in self.fc_list:\n",
    "            output_fin = fc(x)  # Apply each decoder to the concatenated features\n",
    "            # output_fin = output_fin.transpose(1, 2)  # [B, T\n",
    "            outputs.append(output_fin)\n",
    "        outputs = torch.cat(outputs, dim=-1)  # Concatenate outputs for each cluster\n",
    "        return outputs  # Return list of outputs for each cluster\n",
    "\n",
    "class ImprovedConv1DEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder1 = IBNNet(input_dim, hidden_dim)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=5, stride=1, padding=2)  # Downsample by 2\n",
    "        self.encoder2 = IBNNet(256, hidden_dim)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=5, stride=1, padding=2)  # Downsample by 2\n",
    "        self.encoder3 = IBNNet(256, hidden_dim)\n",
    "        self.maxpool3 = nn.MaxPool1d(kernel_size=5, stride=1, padding=2)  # Downsample by 2\n",
    "        self.encoder4 = IBNNet(256, hidden_dim)\n",
    "        self.maxpool4 = nn.MaxPool1d(kernel_size=5, stride=1, padding=2)  # Downsample by 2\n",
    "        # self.encoder5 = EncoderHelper(256, hidden_dim)  # Last encoder without pooling\n",
    "        # self.maxpool5 = nn.MaxPool1d(kernel_size=5, stride=1, padding=2)  # Optional pooling for last encoder\n",
    "        # self.encoder6 = EncoderHelper(256, hidden_dim)  # Additional encoder for deeper features\n",
    "        # self.maxpool6 = nn.MaxPool1d(kernel_size=5, stride=1, padding=2)  # Optional pooling for last encoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # [B, input_dim, T]\n",
    "        enc1 = self.encoder1(x, res=False)  # [B, 256, T]\n",
    "        enc1 = self.maxpool1(enc1)  # Downsample by 2\n",
    "\n",
    "        enc2 = self.encoder2(enc1, res=True) # [B, 256, T]\n",
    "        enc2 = self.maxpool2(enc2)  # Downsample by 2\n",
    "\n",
    "        enc3 = self.encoder3(enc2, res=True) # [B, 256, T]\n",
    "        enc3 = self.maxpool3(enc3)  # Downsample by 2\n",
    "\n",
    "        # enc4 = self.encoder4(enc3, res=True)\n",
    "        # enc4 = self.maxpool4(enc4)  # Downsample by 2\n",
    "\n",
    "        # enc5 = self.encoder5(enc4, res=True)  # [B, 256, T]\n",
    "        # enc5 = self.maxpool5(enc5)\n",
    "\n",
    "        # enc6 = self.encoder6(enc5, res=True)  # [B, 256, T]\n",
    "\n",
    "        return enc3, enc2, enc1  # Return all encoder outputs for skip connections\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super().__init__()\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.transpose(x, self.dim1, self.dim2)\n",
    "    \n",
    "class MCDNILM(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.encoder = ImprovedConv1DEncoder(input_dim, hidden_dim)\n",
    "        self.temporal_lstm = nn.LSTM(258, 128, batch_first=True, bidirectional=True)\n",
    "        self.temporal_ln = nn.LayerNorm(256)\n",
    "        self.attention = FuzzyCMeansClustering(input_dim=256, num_clusters=3)\n",
    "\n",
    "        # Use UNetStyleDecoder for all clusters\n",
    "        self.decoders = nn.ModuleList([\n",
    "            UNetDecoder(256, 3, kernel_size=5, padding=2),  # Long-cycle appliances\n",
    "            UNetDecoder(256, 1, kernel_size=1, padding=0),  # HVAC appliances\n",
    "            UNetDecoder(256, 3, kernel_size=5, padding=2, hidden_dim=80)   # Short-cycle appliances\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, input_dim]\n",
    "        enc3, enc2, enc1 = self.encoder(x)  # [B, C, T] triples\n",
    "\n",
    "        # enc_concat = torch.cat([enc5, x.transpose(1, 2)], dim=1)  # Concatenate all encoder outputs\n",
    "        # enc_concat = enc_concat.transpose(1, 2)  # [B, T, C]\n",
    "        # features, _ = self.temporal_lstm(enc_concat)  # [B, T, C]\n",
    "        # features = self.temporal_ln(features)  # Apply layer normalization\n",
    "        # Prepare features for attention: transpose to [B, T, C]\n",
    "        features = enc3.transpose(1, 2)  # [B, T, C]\n",
    "        # print(features.shape)\n",
    "        # features, _ = self.temporal_lstm(features)  # [B, T, C]\n",
    "        # features = self.temporal_ln(features)\n",
    "        # print(features.shape)\n",
    "        cluster_feats, weights = self.attention(features)  # [B, K, T, C]\n",
    "        # weights = None  # No attention weights needed for UNet-style decoder\n",
    "        outputs = []\n",
    "        for i in range(3):\n",
    "            feat = cluster_feats[:, i].transpose(1, 2)  # [B, C, T]\n",
    "            # Pass feat and encoder skip connections to decoder\n",
    "            # feat = enc3  # without clustering\n",
    "            outputs.append(self.decoders[i](feat, enc2, enc1))\n",
    "        output = torch.cat(outputs, dim=-1)  # Concatenate cluster outputs [B, sum_of_output_dims]\n",
    "        return output, weights\n",
    "\n",
    "# test_data = torch.randn(32, 100, 2)  # Example input: [B, T, D]\n",
    "# model = ImprovedConv1DEncDecNILM2(input_dim=2, hidden_dim=64)\n",
    "# output  = model(test_data)\n",
    "# print(output[0].shape)  # Should print: torch.Size([32, 7]) for 3 clusters with outputs [3, 2, 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
