{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Lambda\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "class InstanceNormalization(Layer):\n",
    "    \"\"\"Instance normalization layer.\n",
    "    Normalize the activations of the previous layer at each step,\n",
    "    i.e. applies a transformation that maintains the mean activation\n",
    "    close to 0 and the activation standard deviation close to 1.\n",
    "    # Arguments\n",
    "        axis: Integer, the axis that should be normalized\n",
    "            (typically the features axis).\n",
    "            \n",
    "            For instance, after a `Conv2D` layer with\n",
    "            `data_format=\"channels_first\"`,\n",
    "            set `axis=1` in `InstanceNormalization`.\n",
    "            Setting `axis=None` will normalize all values in each\n",
    "            instance of the batch.\n",
    "            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "            When the next layer is linear (also e.g. `nn.relu`),\n",
    "            this can be disabled since the scaling\n",
    "            will be done by the next layer.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a Sequential model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
    "        - [Instance Normalization: The Missing Ingredient for Fast Stylization](\n",
    "        https://arxiv.org/abs/1607.08022)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 axis=None,\n",
    "                 epsilon=1e-3,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ndim = len(input_shape)\n",
    "        if self.axis == 0:\n",
    "            raise ValueError('Axis cannot be zero')\n",
    "\n",
    "        if (self.axis is not None) and (ndim == 2):\n",
    "            raise ValueError('Cannot specify axis for rank 1 tensor')\n",
    "\n",
    "        self.input_spec = InputSpec(ndim=ndim)\n",
    "\n",
    "        if self.axis is None:\n",
    "            shape = (1,)\n",
    "        else:\n",
    "            shape = (input_shape[self.axis],)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        reduction_axes = list(range(0, len(input_shape)))\n",
    "\n",
    "        if self.axis is not None:\n",
    "            del reduction_axes[self.axis]\n",
    "\n",
    "        del reduction_axes[0]\n",
    "\n",
    "        mean = K.mean(inputs, reduction_axes, keepdims=True)\n",
    "        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n",
    "        normed = (inputs - mean) / stddev\n",
    "\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        if self.axis is not None:\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        if self.scale:\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            normed = normed * broadcast_gamma\n",
    "        if self.center:\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            normed = normed + broadcast_beta\n",
    "        return normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(InstanceNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def conv_block_seq_res_fixe(inputs, filters, kernel_size, strides, name, bn=True, In=True, ResCon=True):\n",
    "    outputs = tf.keras.layers.Conv1D(64, kernel_size, strides=strides, padding=\"same\", name=\"{}_Conv1D1\".format(name))(inputs)\n",
    "    if bn:\n",
    "        outputs = tf.keras.layers.BatchNormalization(name=\"{}_BatchNorm1\".format(name))(outputs)\n",
    "    outputs = tf.keras.layers.Activation(\"relu\", name=\"{}_ReLU1\".format(name))(outputs)\n",
    "    outputs = tf.keras.layers.Conv1D(64, 1, strides=strides, padding=\"same\", name=\"{}_Conv1D2\".format(name))(outputs)\n",
    "    if bn:\n",
    "        outputs = tf.keras.layers.BatchNormalization(name=\"{}_BatchNorm2\".format(name))(outputs)\n",
    "    outputs = tf.keras.layers.Activation(\"relu\", name=\"{}_ReLU2\".format(name))(outputs)\n",
    "    outputs = tf.keras.layers.Conv1D(256, kernel_size, strides=strides, padding=\"same\", name=\"{}_Conv1D3\".format(name))(outputs)\n",
    "    if bn:\n",
    "        outputs = tf.keras.layers.BatchNormalization(name=\"{}_BatchNorm3\".format(name))(outputs)\n",
    "    # Residual Add\n",
    "    if ResCon:\n",
    "        outputs = tf.keras.layers.Add()([outputs, inputs])\n",
    "    \n",
    "    if In:\n",
    "        outputs = InstanceNormalization(name=\"{}_InstNorm2\".format(name))(outputs)\n",
    "\n",
    "    outputs = tf.keras.layers.Activation(\"relu\", name=\"{}_ReLU3\".format(name))(outputs)\n",
    "    return outputs\n",
    "    \n",
    "\n",
    "def Conv1DTranspose(input_tensor, filters, kernel_size, strides=2, padding='same', activation=None):\n",
    "    \"\"\"\n",
    "        input_tensor: tensor, with the shape (batch_size, time_steps, dims)\n",
    "        filters: int, output dimension, i.e. the output tensor will have the shape of (batch_size, time_steps, filters)\n",
    "        kernel_size: int, size of the convolution kernel\n",
    "        strides: int, convolution step size\n",
    "        padding: 'same' | 'valid'\n",
    "    \"\"\"\n",
    "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(input_tensor)\n",
    "    x = Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), strides=(strides, 1), padding=padding, activation=activation)(x)\n",
    "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Sampling, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mu, z_log_var = inputs\n",
    "        batch_size = tf.shape(z_mu)[0]\n",
    "        latent_dim = tf.shape(z_mu)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, latent_dim))  # Generate noise\n",
    "        z = z_mu + tf.exp(0.5 * z_log_var) * epsilon  # Reparameterization trick\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model, config, width, optimizer):\n",
    "    \n",
    "    config = \"fixe_filter\"\n",
    "    \n",
    "    if model == \"VAE\":\n",
    "        def KL_loss(y_true, y_pred):\n",
    "            # Regularization term\n",
    "            kl_loss = - .5 * tf.reduce_sum(1 + z_log_var - K.square(z_mu) - K.exp(z_log_var), axis=-1)\n",
    "            return kl_loss\n",
    "\n",
    "        def Recon_loss(data_orig, data_reconstructed):\n",
    "            reconstruction_loss = tf.reduce_mean((data_orig - data_reconstructed)**2)\n",
    "            return reconstruction_loss\n",
    "\n",
    "        def vae_loss(data_orig, data_reconstructed):\n",
    "            reconstruction_loss = tf.reduce_mean((data_orig - data_reconstructed)**2)\n",
    "\n",
    "            kl_loss = - .5 * tf.reduce_sum(1 + z_log_var - K.square(z_mu) - K.exp(z_log_var), axis=-1)\n",
    "\n",
    "            return reconstruction_loss + kl_loss\n",
    "            \n",
    "        if config == \"fixe_filter\":\n",
    "            start_filter_num = 256\n",
    "            kernel_size = 3\n",
    "            latent_dim = 1\n",
    "            div = 64\n",
    "\n",
    "            x = tf.keras.Input(shape=(width,1))\n",
    "            print(\"Input shape: \", x.shape)\n",
    "            # eps = tf.keras.Input(shape=(latent_dim,), name=\"eps\")\n",
    "\n",
    "            conv_seq1 = conv_block_seq_res_fixe(x, start_filter_num, kernel_size, 1, \"conv_seq1\", ResCon=False)\n",
    "            pool1 = tf.keras.layers.MaxPooling1D(name=\"pool1\")(conv_seq1)\n",
    "\n",
    "            conv_seq2 = conv_block_seq_res_fixe(pool1, start_filter_num, kernel_size, 1, \"conv_seq2\")\n",
    "            pool2 = tf.keras.layers.MaxPooling1D(name=\"pool2\")(conv_seq2)\n",
    "\n",
    "            conv_seq3 = conv_block_seq_res_fixe(pool2, start_filter_num, kernel_size, 1, \"conv_seq3\")\n",
    "            pool3 = tf.keras.layers.MaxPooling1D(name=\"pool3\")(conv_seq3)\n",
    "\n",
    "            conv_seq4 = conv_block_seq_res_fixe(pool3, start_filter_num, kernel_size, 1, \"conv_seq4\")\n",
    "            pool4 = tf.keras.layers.MaxPooling1D(name=\"pool4\")(conv_seq4)\n",
    "\n",
    "            conv_seq5 = conv_block_seq_res_fixe(pool4, start_filter_num, kernel_size, 1, \"conv_seq5\")\n",
    "            pool5 = tf.keras.layers.MaxPooling1D(name=\"pool5\")(conv_seq5)\n",
    "\n",
    "            conv_seq6 = conv_block_seq_res_fixe(pool5, start_filter_num, kernel_size, 1, \"conv_seq6\", In=False)\n",
    "            pool6 = tf.keras.layers.MaxPooling1D(name=\"pool6\")(conv_seq6)\n",
    "\n",
    "            conv_seq7 = conv_block_seq_res_fixe(pool6, start_filter_num, kernel_size, 1, \"conv_seq7\", In=False)\n",
    "            flatten1 = tf.keras.layers.Flatten()(conv_seq7)\n",
    "\n",
    "            z_mu = tf.keras.layers.Dense(latent_dim, name=\"z_mu\")(flatten1)\n",
    "            z_log_var = tf.keras.layers.Dense(latent_dim, name=\"z_log_var\")(flatten1)\n",
    "\n",
    "            ###############################################################################\n",
    "            # normalize log variance to std dev\n",
    "            # z_sigma = tf.keras.layers.Lambda(lambda t: K.exp(.5*t), name=\"z_sigma\")(z_log_var)\n",
    "            # eps = tf.keras.Input(tensor=K.random_normal(shape=(K.shape(x)[0], latent_dim)), name=\"eps\")\n",
    "            # eps = tf.keras.layers.Lambda(lambda _: tf.random.normal(shape=(tf.shape(x)[0], latent_dim)), name=\"eps\")(x)\n",
    "\n",
    "\n",
    "            # z_eps = tf.keras.layers.Multiply(name=\"z_eps\")([z_sigma, eps])\n",
    "            z = Sampling(name=\"sampling\")([z_mu, z_log_var])\n",
    "            # z = tf.keras.layers.Add(name=\"z\")([z_mu, z_eps])\n",
    "            #latent_conv = tf.keras.layers.Dense(width//64, name=\"latent_conv\")(z)\n",
    "\n",
    "            reshape1 = tf.keras.layers.Reshape([width//div,1], name=\"reshape1\")(z)\n",
    "            \n",
    "            ###############################################################################\n",
    "            #New for conditional VAE\n",
    "            dconv_seq4 = conv_block_seq_res_fixe(reshape1, start_filter_num, kernel_size, 1, \"dconv_seq4\", In=False, ResCon=False)\n",
    "            dconc5 = tf.keras.layers.concatenate([dconv_seq4, conv_seq7], name=\"dconc5\")\n",
    "            deconv1 = Conv1DTranspose(dconc5, start_filter_num, kernel_size=3, strides=2, padding='same')\n",
    "\n",
    "            dconv_seq5 = conv_block_seq_res_fixe(deconv1, start_filter_num, kernel_size, 1, \"dconv_seq5\", In=False)\n",
    "            dconc7 = tf.keras.layers.concatenate([dconv_seq5, conv_seq6], name=\"dconc7\")\n",
    "            deconv2 = Conv1DTranspose(dconc7, start_filter_num, kernel_size=3, strides=2, padding='same')\n",
    "\n",
    "            dconv_seq6 = conv_block_seq_res_fixe(deconv2, start_filter_num, kernel_size, 1, \"dconv_seq6\", In=False)\n",
    "            dconc9 = tf.keras.layers.concatenate([dconv_seq6, conv_seq5], name=\"dconc9\")\n",
    "            deconv3 = Conv1DTranspose(dconc9, start_filter_num, kernel_size=3, strides=2, padding='same')\n",
    "\n",
    "            dconv_seq7 = conv_block_seq_res_fixe(deconv3, start_filter_num, kernel_size, 1, \"dconv_seq7\", In=False)\n",
    "            dconc11 = tf.keras.layers.concatenate([dconv_seq7, conv_seq4], name=\"dconc11\")\n",
    "            deconv4 = Conv1DTranspose(dconc11, start_filter_num, kernel_size=3, strides=2, padding='same')\n",
    "\n",
    "            dconv_seq8 = conv_block_seq_res_fixe(deconv4, start_filter_num, kernel_size, 1, \"dconv_seq8\", In=False)\n",
    "            dconc13 = tf.keras.layers.concatenate([dconv_seq8, conv_seq3], name=\"dconc13\")\n",
    "            deconv5 = Conv1DTranspose(dconc13, start_filter_num, kernel_size=3, strides=2, padding='same')\n",
    "\n",
    "            dconv_seq9 = conv_block_seq_res_fixe(deconv5, start_filter_num, kernel_size, 1, \"dconv_seq9\", In=False)\n",
    "            dconc15 = tf.keras.layers.concatenate([dconv_seq9, conv_seq2], name=\"dconc15\")\n",
    "            deconv6 = Conv1DTranspose(dconc15, start_filter_num, kernel_size=3, strides=2, padding='same')\n",
    "\n",
    "            dconv_seq10 = conv_block_seq_res_fixe(deconv6, start_filter_num, kernel_size, 1, \"dconv_seq10\", In=False)\n",
    "            dconc17 = tf.keras.layers.concatenate([dconv_seq10, conv_seq1], name=\"dconc17\")\n",
    "\n",
    "            x_pred = tf.keras.layers.Conv1D(1, 3, padding=\"same\", activation=\"relu\", name=\"x_pred\")(dconc17)\n",
    "\n",
    "            model = tf.keras.Model(inputs=x, outputs=x_pred)\n",
    "            model.summary()\n",
    "            \n",
    "        model.compile(optimizer=optimizer, loss=vae_loss, metrics=[KL_loss, vae_loss, \"mean_absolute_error\"])\n",
    "        \n",
    "        \n",
    "    return model\n",
    "\n",
    "class CustomStopper(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='auto', start_epoch=5):\n",
    "        super().__init__(monitor=monitor, min_delta=min_delta, patience=patience, verbose=verbose, mode=mode)\n",
    "        self.start_epoch = start_epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"On epoch End!\")\n",
    "        if epoch > self.start_epoch:\n",
    "            super().on_epoch_end(epoch, logs)\n",
    "            print(\"On epoch End after starting point!\")\n",
    "    \n",
    "class AdditionalValidationSets(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) not in [2, 3]:\n",
    "                raise ValueError()\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) == 3:\n",
    "                validation_data, validation_targets, validation_set_name = validation_set\n",
    "                sample_weights = None\n",
    "            elif len(validation_set) == 4:\n",
    "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            results = self.model.evaluate(x=validation_data,\n",
    "                                          y=validation_targets,\n",
    "                                          verbose=self.verbose,\n",
    "                                          sample_weight=sample_weights,\n",
    "                                          batch_size=self.batch_size)\n",
    "\n",
    "            for i, result in enumerate(results):\n",
    "                if i == 0:\n",
    "                    valuename = validation_set_name + '_loss'\n",
    "                else:\n",
    "                    valuename = validation_set_name + '_' + str(self.model.metrics[i-1].name)\n",
    "                self.history.setdefault(valuename, []).append(result)\n",
    "\n",
    "def reconstruct(y, width, strides, merge_type=\"mean\"):\n",
    "    \n",
    "    len_total = width+(y.shape[0]-1)*strides\n",
    "    depth = width//strides\n",
    "    \n",
    "    yr = np.zeros([len_total, depth])\n",
    "    yr[:] = np.nan\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        for d in range(depth):\n",
    "            yr[i*strides+(d*strides):i*strides+((d+1)*strides),d] = y[i,d*strides:(d+1)*strides,0]\n",
    "    if merge_type == \"mean\":\n",
    "        yr = np.nanmean(yr, axis=1)\n",
    "    else:\n",
    "        yr = np.nanmedian(yr, axis=1)\n",
    "    \n",
    "    return yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
