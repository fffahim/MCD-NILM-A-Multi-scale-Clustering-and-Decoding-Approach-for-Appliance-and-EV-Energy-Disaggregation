{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8653cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models/MCD-NILM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, X, y, long_tern, seasonal, short_term, train=True):\n",
    "        \"\"\"\n",
    "        Dataset for energy disaggregation\n",
    "        \n",
    "        Args:\n",
    "            X: Total energy consumption data of shape (n_samples, sequence_length, 1)\n",
    "            y: Individual appliance consumption data of shape (n_samples, sequence_length, n_appliances)\n",
    "            train: Whether this is training data (for potential data augmentation)\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.long_term = torch.tensor(long_tern, dtype=torch.float32)\n",
    "        self.seasonal = torch.tensor(seasonal, dtype=torch.float32)\n",
    "        self.short_term = torch.tensor(short_term, dtype=torch.float32)\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_sample = self.X[idx]\n",
    "        y_sample = self.y[idx]\n",
    "        long_term_sample = self.long_term[idx]\n",
    "        seasonal_sample = self.seasonal[idx]\n",
    "        short_term_sample = self.short_term[idx]\n",
    "        \n",
    "        return X_sample, y_sample, long_term_sample, seasonal_sample, short_term_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3925871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(df, cols, mean_std=None):\n",
    "    \"\"\"\n",
    "    Standardize input and target columns in the DataFrame\n",
    "    \"\"\"\n",
    "    if mean_std is None:\n",
    "        mean_std = {}\n",
    "    for col in cols:\n",
    "        if mean_std is None:\n",
    "            mean_std[col] = (df[col].mean(), df[col].std())\n",
    "        df[col] = (df[col] - mean_std[col][0]) / mean_std[col][1]\n",
    "        # mean_std[col] = (df[col].min(), df[col].max())\n",
    "        # df[col] = (df[col] - mean_std[col][0]) / (mean_std[col][1] - mean_std[col][0])\n",
    "    \n",
    "    return torch.tensor(df.values), mean_std\n",
    "\n",
    "def get_mean_std(df, cols):\n",
    "    \"\"\"\n",
    "    Get mean and standard deviation for the specified columns in the DataFrame\n",
    "    \"\"\"\n",
    "    mean_std = {}\n",
    "    for col in cols:\n",
    "        mean_std[col] = (df[col].mean(), df[col].std())\n",
    "    \n",
    "    return mean_std\n",
    "\n",
    "def inverse_standardize_data(df, mean_std):\n",
    "    \"\"\"\n",
    "    Inverse standardize the DataFrame columns\n",
    "    \"\"\"\n",
    "    for col, (mean, std) in mean_std.items():\n",
    "        df[col] = df[col] * std + mean\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data(df_input, df_targets, seq_len, stride):\n",
    "    \"\"\"\n",
    "    Prepare data for training and testing\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    long_term = []\n",
    "    seasonal = []\n",
    "    short_term = []\n",
    "\n",
    "    for i in range(0, len(df_input) - seq_len, stride):\n",
    "        seq = df_input[i:i + seq_len]\n",
    "        lt = df_input[i:i + seq_len]\n",
    "        se = df_input[i:i + 10]\n",
    "        st = df_input[i:i + 5]\n",
    "\n",
    "        long_term.append(lt)\n",
    "        seasonal.append(se)\n",
    "        short_term.append(st)\n",
    "        target = df_targets[i: i + seq_len]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    long_term = np.array(long_term)\n",
    "    seasonal = np.array(seasonal)\n",
    "    short_term = np.array(short_term)\n",
    "    residuals_input = [long_term, seasonal, short_term]\n",
    "    sequences = np.array(sequences)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return sequences, targets, residuals_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(y, width=30, strides=1, merge_type=\"mean\"):\n",
    "    \n",
    "    len_total = width+(y.shape[0]-1)*strides\n",
    "    depth = width//strides\n",
    "    \n",
    "    yr = np.zeros([len_total, depth])\n",
    "    yr[:] = np.nan\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        for d in range(depth):\n",
    "            yr[i*strides+(d*strides):i*strides+((d+1)*strides),d] = y[i,d*strides:(d+1)*strides,0]\n",
    "    if merge_type == \"mean\":\n",
    "        yr = np.nanmean(yr, axis=1)\n",
    "    else:\n",
    "        yr = np.nanmedian(yr, axis=1)\n",
    "    \n",
    "    return yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f06dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/661_compressed.csv')\n",
    "# data_test = pd.read_csv('../data/661_compressed.csv')\n",
    "# data['weekend'] = data['local_15min'].apply(lambda x: 1 if pd.to_datetime(x).dayofweek >= 5 else 0)\n",
    "# data['hour'] = data['local_15min'].apply(lambda x: pd.to_datetime(x).hour)\n",
    "# data['minute'] = data['local_15min'].apply(lambda x: pd.to_datetime(x).minute)\n",
    "# # # morning from 6 -12, afternoon from 12-17, evening from 17-20, night from 20-6\n",
    "# # data['time_of_day'] = data['hour'].apply(lambda x: 0 if 6 <= x < 12 else (1 if 12 <= x < 17 else (2 if 17 <= x < 20 else 3)))\n",
    "# data['day_of_week'] = data['local_15min'].apply(lambda x: pd.to_datetime(x).dayofweek + 1)  # Monday=1, Sunday=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column for season in usa -> summar -> may - august\n",
    "# data['season'] = data['local_15min'].apply(lambda x: 1 if 5 <= pd.to_datetime(x).month <= 8 else 0)\n",
    "data['season'] = data['local_15min'].apply(lambda x: 0 if 3 <= pd.to_datetime(x).month < 6 else (1 if 6 <= pd.to_datetime(x).month < 9 else (2 if 9 <= pd.to_datetime(x).month < 12 else 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col = ['total_usage']\n",
    "test_col = ['refrigerator', 'ev_car', 'others', 'air', 'microwave', 'dishwasher', 'clotheswasher']\n",
    "# data['total_usage'] = data['total_usage'] - data['ev_car']\n",
    "data[train_col + test_col] = data[train_col + test_col]\n",
    "data[train_col + test_col] = data[train_col + test_col]\n",
    "# data['day_of_week'] = pd.to_datetime(data['timestamp']).dt.dayofwe\n",
    "# train_col = train_col + ['weekend', 'hour', 'day_of_week', 'season', 'minute']  # Add time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.7)\n",
    "test_size = len(data) - train_size\n",
    "X_train = data.loc[:train_size, train_col]\n",
    "y_train = data.loc[:train_size, test_col]\n",
    "X_test = data.loc[train_size:, train_col]\n",
    "y_test = data.loc[train_size:, test_col]\n",
    "\n",
    "mean_std_X = get_mean_std(data, train_col)\n",
    "mean_std_Y = get_mean_std(data, test_col)\n",
    "\n",
    "# mean_std_X_test = get_mean_std(data_test, train_col)\n",
    "# mean_std_Y_test = get_mean_std(data_test, test_col)\n",
    "\n",
    "x_train_standardized, scaler_x_train = standardize_data(X_train.copy(), train_col, mean_std_X)\n",
    "y_train_standardized, scaler_y_train = standardize_data(y_train.copy(), test_col, mean_std_Y)\n",
    "\n",
    "x_test_standardized, scaler_x_test = standardize_data(X_test.copy(), train_col, mean_std_X)\n",
    "y_test_standardized, scaler_y_test = standardize_data(y_test.copy(), test_col, mean_std_Y)\n",
    "\n",
    "x_processed_train, y_processed_train, residual_train = prepare_data(\n",
    "    x_train_standardized, y_train_standardized, seq_len=30, stride=1)\n",
    "\n",
    "x_processed_test, y_processed_test, residual_test = prepare_data(\n",
    "    x_test_standardized, y_test_standardized, seq_len=30, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82497d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EnergyDataset(x_processed_train, y_processed_train, residual_train[0], residual_train[1], residual_train[2], train=True)\n",
    "test_data = EnergyDataset(x_processed_test, y_processed_test, residual_test[0], residual_test[1], residual_test[2], train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ee658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def entropy_loss(weights):\n",
    "    entropy = -torch.sum(weights * torch.log(weights + 1e-8), dim=-1)\n",
    "    return entropy.mean()\n",
    "\n",
    "def kl_uniform_loss(weights):\n",
    "    B, T, K = weights.shape\n",
    "    uniform = torch.full_like(weights, 1.0 / K)\n",
    "    return F.kl_div(weights.log(), uniform, reduction='batchmean')\n",
    "\n",
    "model = MCDNILM(input_dim=1, hidden_dim=64)\n",
    "# model.apply(init_weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "total_time = 0.0\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    for inputs, targets, _, _, _ in train_dataloader:\n",
    "        inputs = inputs.to(device)  # Shape: (batch_size, 1, window_size)\n",
    "        targets = targets.to(device)  # Shape: (batch_size, num_appliances)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, weights = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss2 = F.l1_loss(outputs, targets)\n",
    "        loss3 = entropy_loss(weights)  # Calculate entropy loss\n",
    "        loss = loss +  loss2 + 0.01 * loss3# Combine MSE and L\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    end_time = time.time()\n",
    "    total_time += end_time - start_time\n",
    "    epoch_loss = running_loss / len(train_data)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Time: {end_time - start_time:.2f}s')\n",
    "print(\"🔁 Peak GPU memory used during 1 epoch (training):\", torch.cuda.max_memory_allocated() / (1024 ** 2), \"MB\")\n",
    "print(f'Average time per epoch: {total_time / num_epochs:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19037a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "# testing loop\n",
    "model.eval()\n",
    "running_loss = 0.0\n",
    "targets_list = []\n",
    "outputs_list = []\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss\n",
    "\n",
    "for inputs, targets, _, _, _ in test_dataloader:\n",
    "    inputs = inputs.to(device)  # Shape: (batch_size, 1, window_size)\n",
    "    targets = targets.to(device)  # Shape: (batch_size, num_appliances)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "    targets_list.append(targets.cpu().numpy())\n",
    "    outputs_list.append(outputs.cpu().numpy())\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "mem_after = process.memory_info().rss\n",
    "print(\"🧠 PyTorch inference memory (CPU):\", (mem_after - mem_before) / (1024 ** 2), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdfcb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_array = np.concatenate(targets_list, axis=0)\n",
    "outputs_array = np.concatenate(outputs_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb373814",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_array_reverse = targets_array.copy()\n",
    "outputs_array_reverse = outputs_array.copy()\n",
    "\n",
    "for index, key in enumerate(scaler_y_test):\n",
    "    # Reverse scale in place for each feature dimension\n",
    "    targets_array_reverse[:, :, index] = targets_array[:, :, index] * scaler_y_test[key][1] + scaler_y_test[key][0]\n",
    "    outputs_array_reverse[:, :, index] = outputs_array[:, :, index] * scaler_y_test[key][1] + scaler_y_test[key][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579cc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_array_reconstruct = []\n",
    "outputs_array_reconstruct = []\n",
    "\n",
    "for i in range(targets_array_reverse.shape[-1]):\n",
    "    targets_array_reconstruct.append(reconstruct(targets_array_reverse[:, :, i].reshape(*targets_array_reverse[:, :, 0].shape, 1), width=30, strides=1, merge_type=\"mean\"))\n",
    "    outputs_array_reconstruct.append(reconstruct(outputs_array_reverse[:, :, i].reshape(*targets_array_reverse[:, :, 0].shape, 1), width=30, strides=1, merge_type=\"mean\"))\n",
    "targets_array_reconstruct = np.array(targets_array_reconstruct).T\n",
    "outputs_array_reconstruct = np.array(outputs_array_reconstruct).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_targets_tensor = torch.tensor(targets_array_reconstruct, dtype=torch.float32)\n",
    "inverse_outputs_tensor = torch.tensor(outputs_array_reconstruct, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero all negative values in the inverse outputs tensor\n",
    "inverse_outputs_tensor[inverse_outputs_tensor < 0] = 0.0\n",
    "# zero all negative values in the inverse targets tensor\n",
    "inverse_targets_tensor[inverse_targets_tensor < 0] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dddae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the results for the first appliance (e.g., refrigerator)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(inverse_targets_tensor[:500000, 0], label='Actual Refrigerator Consumption', color='blue')\n",
    "plt.plot(inverse_outputs_tensor[:500000, 0], label='Predicted Refrigerator Consumption', color='orange')\n",
    "plt.title('Refrigerator Consumption Prediction')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Power Consumption (kW)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65bfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "for i, val in enumerate(test_col):\n",
    "    ground_truth = inverse_targets_tensor[:, i]\n",
    "    predicted = inverse_outputs_tensor[:, i]\n",
    "    mae_loss_value = np.abs(ground_truth - predicted).mean()\n",
    "    # mae_loss_value = mae_loss(ground_truth, predicted)\n",
    "\n",
    "    num = np.abs(ground_truth - predicted).sum(axis=0)\n",
    "    den = 2*ground_truth.sum(axis=0)\n",
    "    eac = (1 - num/den)\n",
    "\n",
    "    nde = torch.sum((ground_truth - predicted) ** 2) / torch.sum((ground_truth ** 2), axis=0)\n",
    "    print(f'Appliance {val} - MAE Loss: {mae_loss_value:.4f}, EAC: {eac:.4f}, NDE: {nde:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_test = pd.DataFrame(inverse_outputs_tensor.numpy(), columns=test_col)\n",
    "dataframe_ground_truth = pd.DataFrame(inverse_targets_tensor.numpy(), columns=test_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a95b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_test.to_csv('data/8156_ev_predict_our.csv', index=False)\n",
    "dataframe_ground_truth.to_csv('data/8156_ev_ground_truth_our.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
