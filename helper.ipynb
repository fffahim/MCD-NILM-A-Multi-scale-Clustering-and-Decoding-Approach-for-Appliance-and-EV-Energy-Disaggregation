{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, col, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.col = col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx : idx + self.seq_len]\n",
    "        return sample[:, : self.col], sample[-1, self.col :].unsqueeze(0), sample[-1, : self.col].unsqueeze(0)\n",
    "\n",
    "class PrepareData:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def load_data(self, num_of_houses):\n",
    "        # read from train list and concatenate the data\n",
    "        for i in range(len(num_of_houses)):\n",
    "            data = pd.read_csv(self.config.data_path + str(num_of_houses[i]) + '_compressed.csv')\n",
    "            if i == 0:\n",
    "                all_data = data\n",
    "            else:\n",
    "                all_data = pd.concat([all_data, data], axis=0)\n",
    "        return all_data\n",
    "    \n",
    "    # write a function using MinMax scalar to normalize the data\n",
    "    def normalize_data(self, data, normalize = True, scaler = None):\n",
    "        if normalize:\n",
    "            #z score normalization\n",
    "            if scaler == None:\n",
    "                scaler = StandardScaler()\n",
    "                data = scaler.fit_transform(data)\n",
    "                print(\"1\")\n",
    "            else:\n",
    "                data = scaler.transform(data)\n",
    "                print(\"2\")\n",
    "        else:\n",
    "            scaler = None\n",
    "            # data = scaler.fit_transform(data)\n",
    "            data = data.to_numpy()\n",
    "            # scaler = 0\n",
    "        return data, scaler\n",
    "    \n",
    "    def segment_ev_load_data(self, data, window_length, num_of_houses, col, single_output = False, model_type = 'regression'):\n",
    "        \n",
    "        data = torch.from_numpy(data).to(self.config.device).float()\n",
    "        segmented_data = []\n",
    "        ground_truth = []\n",
    "        for i in range(len(num_of_houses)):\n",
    "            per_house_data = data[data[:, 0] == num_of_houses[i]]\n",
    "            # Calculate the number of segments\n",
    "            num_segments = (len(per_house_data) - window_length) + 1\n",
    "            # Perform sliding window segmentation\n",
    "            for start in range(num_segments):\n",
    "                segmented_data.append(per_house_data[start:start + window_length, 1:col])\n",
    "                if single_output:\n",
    "                    ground_truth.append(per_house_data[start + window_length - 1, col:])\n",
    "                else:\n",
    "                    ground_truth.append(per_house_data[start:start + window_length, col:])\n",
    "\n",
    "        # Perform sliding window segmentation\n",
    "        # for start in range(num_segments):\n",
    "        #     segmented_data = torch.cat(segmented_data, data[start:start + window_length, 1:])\n",
    "        #     ground_truth = torch.cat(ground_truth, data[start:start + window_length, 0:1])\n",
    "\n",
    "        # segmented_3d = segmented_data.reshape(segmented_data.shape[0], segmented_data.shape[1], -1)\n",
    "        # ground_truth = ground_truth.reshape(ground_truth.shape[0], ground_truth.shape[1], -1)\n",
    "\n",
    "        segmented_data = torch.stack(segmented_data)\n",
    "        ground_truth = torch.stack(ground_truth)\n",
    "\n",
    "        segmented_data = segmented_data[: -(segmented_data.shape[0] % self.config.batch_size)]\n",
    "        ground_truth = ground_truth[: -(ground_truth.shape[0] % self.config.batch_size)]\n",
    "        return segmented_data, ground_truth\n",
    "        \n",
    "    #write a function for train test loader\n",
    "    def get_data_loader(self, data_preprocess, columns, seen = False, model_type = 'regression'):\n",
    "        data = self.load_data(self.config.train)\n",
    "        data = data_preprocess.preprocess_data(data)\n",
    "\n",
    "        if seen:\n",
    "            train_data, test_data = self.split_train_test(data)\n",
    "            # return train_data, train_data, test_data, test_data, None, None\n",
    "            id_col_train = train_data['dataid'].to_numpy().reshape(-1, 1)\n",
    "            id_col_test = test_data['dataid'].to_numpy().reshape(-1, 1)\n",
    "            train_data, scaler_train = self.normalize_data(train_data[columns], True)\n",
    "            test_data, scalar_test = self.normalize_data(test_data[columns], True, scaler_train)\n",
    "            train_data = np.concatenate((id_col_train, train_data), axis=1)\n",
    "            test_data = np.concatenate((id_col_test, test_data), axis=1) \n",
    "            train_data, train_ground_truth = self.segment_ev_load_data(train_data, self.config.lag_size, self.config.train, self.config.col, self.config.single_output, model_type)\n",
    "            test_data, test_ground_truth = self.segment_ev_load_data(test_data, self.config.lag_size, self.config.test, self.config.col, self.config.single_output, model_type)\n",
    "\n",
    "            if model_type == 'classification':\n",
    "                train_ground_truth = train_ground_truth.unsqueeze(1)\n",
    "                test_ground_truth = test_ground_truth.unsqueeze(1)\n",
    "        \n",
    "        else:\n",
    "            id_col = data['dataid'].to_numpy().reshape(-1, 1)\n",
    "            train_data, scaler_train = self.normalize_data(data[columns], False)\n",
    "            train_data = np.concatenate((id_col, train_data), axis=1)\n",
    "            train_data, train_ground_truth = self.segment_ev_load_data(train_data, self.config.lag_size, self.config.train, self.config.col, self.config.single_output, model_type)\n",
    "\n",
    "            data = self.load_data(self.config.test)\n",
    "            data = data_preprocess.preprocess_data(data)\n",
    "            id_col = data['dataid'].to_numpy().reshape(-1, 1)\n",
    "            test_data, scalar_test = self.normalize_data(data[columns], False)\n",
    "            test_data = np.concatenate((id_col, test_data), axis=1)\n",
    "            test_data, test_ground_truth = self.segment_ev_load_data(test_data, self.config.lag_size, self.config.test, self.config.col, self.config.single_output, model_type)\n",
    "        # test_data, test_ground_truth = self.segment_ev_load_data(test_data, self.config.lag_size)\n",
    "\n",
    "        # train_dataset = CustomDataset(train_data, train_ground_truth)\n",
    "        # test_dataset = CustomDataset(test_data, test_ground_truth)\n",
    "\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=False)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, shuffle=False)\n",
    "\n",
    "        return train_data, train_ground_truth, test_data, test_ground_truth, scaler_train, scalar_test\n",
    "        # return train_loader, test_loader, scaler_train, scalar_test\n",
    "    \n",
    "    def split_train_test(self, data):\n",
    "        train_data = data[data['local_15min'] < self.config.split_date]\n",
    "        test_data = data[data['local_15min'] >= self.config.split_date]\n",
    "        return train_data, test_data\n",
    "    \n",
    "    # write a function using dataloader function\n",
    "    def get_data_loader_unet(self, data_preprocess, columns):\n",
    "        data = self.load_data(self.config.train)\n",
    "        data = data_preprocess.preprocess_data(data)\n",
    "\n",
    "        train_data, test_data = self.split_train_test(data)\n",
    "\n",
    "        train_data, scaler_train = self.normalize_data(train_data[columns], True)\n",
    "        test_data, scalar_test = self.normalize_data(test_data[columns], True, scaler_train)\n",
    "\n",
    "        train_data = torch.tensor(train_data, dtype=torch.float32).to(self.config.device)\n",
    "        test_data = torch.tensor(test_data, dtype=torch.float32).to(self.config.device)\n",
    "\n",
    "        train_dataset = CustomDataset(train_data, self.config.col, self.config.lag_size)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "        test_dataset = CustomDataset(test_data, self.config.col, self.config.lag_size)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=self.config.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "        return train_dataloader, test_dataloader, scaler_train, scalar_test\n",
    "    \n",
    "class DatasetForQuantile(torch.utils.data.Dataset):\n",
    "    def __init__(self,  inputs, targets, model_type, seq_len=99, target_index=None):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.model_type = model_type\n",
    "        seq_len = seq_len \n",
    "        self.seq_len = seq_len\n",
    "        self.len = self.inputs.shape[0] - self.seq_len\n",
    "        self.target_index = target_index\n",
    "        self.indices = np.arange(self.inputs.shape[0])\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.len\n",
    "    \n",
    "    def get_sample(self, index):\n",
    "        indices = self.indices[index : index + self.seq_len]\n",
    "        inds_inputs=sorted(indices[:self.seq_len])\n",
    "        if self.model_type == 'seq2seq':\n",
    "            inds_targs=sorted(indices[:self.seq_len])\n",
    "        else:\n",
    "            if self.target_index is not None:\n",
    "                inds_targs = sorted(indices[self.target_index - 1:self.target_index])\n",
    "            else:\n",
    "                inds_targs=sorted(indices[self.seq_len-1:self.seq_len])\n",
    "\n",
    "        return self.inputs[inds_inputs], self.targets[inds_targs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs, target = self.get_sample(index)\n",
    "        # check if tensor of input is 2d\n",
    "        if len(inputs.shape) == 2:\n",
    "            inputs = torch.tensor(inputs).float()\n",
    "        else:\n",
    "            inputs = torch.tensor(inputs).unsqueeze(-1).float()\n",
    "        \n",
    "        return inputs, torch.tensor(target).float().squeeze()\n",
    "\n",
    "class PrepareDataForQuantile:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config\n",
    "\n",
    "    def normalize_data(self, data, type = 'minmax', normalize = True, scaler = None):\n",
    "        if normalize:\n",
    "            #z score normalization\n",
    "            if scaler == None:\n",
    "                if type == 'minmax':\n",
    "                    scaler = MinMaxScaler()\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                data = scaler.fit_transform(data)\n",
    "            else:\n",
    "                data = scaler.transform(data)\n",
    "        else:\n",
    "            scaler = None\n",
    "            # data = scaler.fit_transform(data)\n",
    "            data = data.to_numpy()\n",
    "            # scaler = 0\n",
    "        return data, scaler\n",
    "    \n",
    "    def spilit_data(self, data):\n",
    "        split_1 = int(0.60 * len(data))\n",
    "        split_2 = int(0.70 * len(data))\n",
    "        train = data[:split_1]\n",
    "        validation = data[split_1:split_2]\n",
    "        test = data[split_2:]\n",
    "        return train, validation, test\n",
    "    \n",
    "    def prepare_dataloader(self, x, y, seq_len, target_index=None):\n",
    "        x_train, x_val, x_test = self.spilit_data(x)\n",
    "        y_train, y_val, y_test = self.spilit_data(y)\n",
    "\n",
    "        train_dataloader = DataLoader(DatasetForQuantile(x_train, y_train, self.config.model_type, seq_len, target_index), batch_size=self.config.batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "        test_dataloader = DataLoader(DatasetForQuantile(x_test, y_test, self.config.model_type, seq_len, target_index), batch_size=self.config.batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "        val_dataloader = DataLoader(DatasetForQuantile(x_val, y_val, self.config.model_type, seq_len, target_index), batch_size=self.config.batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "        return train_dataloader, test_dataloader, val_dataloader\n",
    "    \n",
    "    def get_data_loaders(self, start_col, end_col, seq_len):\n",
    "        #read numpy data\n",
    "        x, y = self.load_data()\n",
    "        y = self.get_specific_col_data(y, start_col, end_col)\n",
    "\n",
    "        train_dataloader, test_dataloader, val_dataloader = self.prepare_dataloader(x, y, seq_len)\n",
    "\n",
    "        #unseen test data\n",
    "        x_unseen, y_unseen = self.load_data_unseen()\n",
    "        y_unseen = self.get_specific_col_data(y_unseen, start_col, end_col)\n",
    "        _, test_dataloader, val_dataloader = self.prepare_dataloader(x_unseen, y_unseen, seq_len)\n",
    "\n",
    "        return train_dataloader, test_dataloader, val_dataloader\n",
    "    \n",
    "    def load_data(self):\n",
    "        x = np.load(\"Dataset/pecan_street/austin/npy_dataset/8156_input_with_ev_unet.npy\")\n",
    "        y = np.load(\"Dataset/pecan_street/austin/npy_dataset/8156_target_with_ev_unet.npy\")\n",
    "        return x, y\n",
    "    \n",
    "    def load_data_unseen(self):\n",
    "        x = np.load(\"Dataset/pecan_street/austin/npy_dataset/661_input_with_ev_unet.npy\")\n",
    "        y = np.load(\"Dataset/pecan_street/austin/npy_dataset/661_target_with_ev_unet.npy\")\n",
    "        return x, y\n",
    "    \n",
    "    def get_specific_col_data(self, y, start_col, end_col):\n",
    "        y = y[:, start_col:end_col]\n",
    "        return y\n",
    "    \n",
    "    def get_data_for_cnn(self, data, input_cols, output_cols, seq_len):\n",
    "        x = self.get_input_features(data[input_cols])\n",
    "        y = data[output_cols]\n",
    "        x, scalar_x = self.normalize_data(x, type=\"sd\")\n",
    "        y, scalar_y = self.normalize_data(y, type=\"sd\")\n",
    "\n",
    "        train_dataloader, test_dataloader, val_dataloader = self.prepare_dataloader(x, y, seq_len, self.config.output_col_num)\n",
    "        return train_dataloader, test_dataloader, val_dataloader, scalar_x, scalar_y\n",
    "        \n",
    "    def get_input_features(self, data):\n",
    "        data['local_15min'] = pd.to_datetime(data['local_15min'], utc=True)\n",
    "        data['hour'] = data['local_15min'].dt.hour\n",
    "        data['minute'] = data['local_15min'].dt.minute\n",
    "        hour_of_day_tensor = torch.tensor(data['hour'].values, dtype=torch.float32)\n",
    "        data['sine_h'] = torch.sin(2 * torch.pi * hour_of_day_tensor / 24)\n",
    "        data['cos_h'] = torch.cos(2 * torch.pi * hour_of_day_tensor / 24)\n",
    "        minute_of_day_tensor = torch.tensor(data['minute'].values, dtype=torch.float32)\n",
    "        data['sine_m'] = torch.sin(2 * torch.pi * minute_of_day_tensor / 60)\n",
    "        data['cos_m'] = torch.cos(2 * torch.pi * minute_of_day_tensor / 60)\n",
    "        data.drop(['local_15min', 'hour', 'minute'], axis=1, inplace=True)\n",
    "        return data\n",
    "    \n",
    "    def get_data_for_vae(self, start_col, end_col, width, strides, test_from=0, set_type=\"both\"):\n",
    "\n",
    "        def seq_dataset(x, y, width, stride):\n",
    "            x_ = []\n",
    "            y_ = []\n",
    "\n",
    "            for t in range(0, x.shape[0]-width, stride):\n",
    "                x_.append(x[t:t+width])\n",
    "                y_.append(y[t:t+width])\n",
    "\n",
    "            x_ = np.array(x_).reshape([-1, width, 1])\n",
    "            y_ = np.array(y_).reshape([-1, width, 1])\n",
    "\n",
    "            return x_, y_\n",
    "\n",
    "        def select_ratio(x, y, ratio, set_type, test_from=0):\n",
    "            num_data = x.shape[0]\n",
    "            if set_type == \"train\":\n",
    "                ind = np.random.permutation(num_data)\n",
    "            else:\n",
    "                ind = np.arange(num_data)\n",
    "            \n",
    "            min_data = int(num_data*test_from)\n",
    "            max_data = int(num_data*ratio)\n",
    "\n",
    "            if ratio == 0:\n",
    "                return x[ind[0:1]], y[ind[0:1]]\n",
    "            else:\n",
    "                return x[ind[:max_data]], y[ind[:max_data]]\n",
    "            \n",
    "        def split_train_test(x, y, ratio):\n",
    "            # num_data = x.shape[0]\n",
    "            # ind = np.random.permutation(num_data)\n",
    "            # min_data = int(num_data*ratio)\n",
    "            # return x[ind[:min_data]], y[ind[:min_data]], x[ind[min_data:]], y[ind[min_data:]]\n",
    "            split_1 = int(ratio * len(x))\n",
    "            x_train = x[:split_1]\n",
    "            x_test = x[split_1:]\n",
    "            y_train = y[:split_1]\n",
    "            y_test = y[split_1:]\n",
    "            \n",
    "            return x_train, y_train, x_test, y_test\n",
    "\n",
    "        def create_dataset(width, strides, set_type, test_from=0):\n",
    "\n",
    "            x, y = self.load_data() # Load complete dataset\n",
    "            y = self.get_specific_col_data(y, start_col, end_col) # Select the appliance\n",
    "            x_train, y_train, _, _ = split_train_test(x, y, 0.7) # Split the dataset in train and test\n",
    "\n",
    "            x_unseen, y_unseen = self.load_data_unseen() # Load unseen dataset\n",
    "            y_unseen = self.get_specific_col_data(y_unseen, start_col, end_col) # Select the appliance\n",
    "            _, _, x_test, y_test = split_train_test(x_unseen, y_unseen, 0.7) # Split the unseen dataset in train and test\n",
    "\n",
    "            print(\"x_train shape : \", x_train.shape)\n",
    "            print(\"y_train shape : \", y_train.shape)\n",
    "            print(\"x_test shape : \", x_test.shape)\n",
    "            print(\"y_test shape : \", y_test.shape)\n",
    "            x_train, y_train = seq_dataset(x_train, y_train, width, strides) # Divide dataset in window\n",
    "            x_test, y_test = seq_dataset(x_test, y_test, width, strides) # Divide dataset in window\n",
    "            # for h, r in zip(dataset[set_type][\"house\"], dataset[set_type][\"ratio\"]):\n",
    "                \n",
    "            #     if set_type == \"test\":\n",
    "            #         x_r, y_r = select_ratio(x_, y_, r, set_type, test_from=test_from)\n",
    "            #     else:\n",
    "            #         x_r, y_r = select_ratio(x_, y_, r, set_type)# Select the proportion needed\n",
    "\n",
    "            #     print(\"Total house {} : x:{}, y:{}\".format(h, x_.shape, y_.shape))\n",
    "            #     print(\"Ratio house {} : {}, x:{}, y:{}\".format(h, r, x_r.shape, y_r.shape))\n",
    "\n",
    "            #     x_tot = np.vstack([x_tot, x_r])\n",
    "            #     y_tot = np.vstack([y_tot, y_r])\n",
    "\n",
    "            # print(\"Complete dataset : x:{}, y{}\".format(x_tot.shape, y_tot.shape))\n",
    "\n",
    "            return x_train, y_train, x_test, y_test\n",
    "\n",
    "        ###############################################################################\n",
    "        # Load dataset\n",
    "        ###############################################################################\n",
    "        width = width\n",
    "        stride = strides\n",
    "\n",
    "        x_train, y_train, x_test, y_test = create_dataset(width, stride, set_type, test_from=test_from)\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "        \n",
    "class QuantileLoss(torch.nn.Module):\n",
    "    def __init__(self, quantiles=[0.0025,0.1, 0.5, 0.9, 0.975]):\n",
    "        self.quantiles = quantiles\n",
    "        super().__init__() \n",
    "    def forward(self, inputs, targets):\n",
    "        targets = targets.unsqueeze(1).expand_as(inputs)\n",
    "        quantiles = torch.tensor(self.quantiles).float().to(targets.device)\n",
    "        error = (targets - inputs).permute(0,2,1)\n",
    "        loss = torch.max(quantiles*error, (quantiles-1)*error)\n",
    "        return loss.mean()\n",
    "    \n",
    "class BinaryDiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss of binary class\n",
    "    Args:\n",
    "        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n",
    "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        predict: A tensor of shape [N, *]\n",
    "        target: A tensor of shape same with predict\n",
    "        reduction: Reduction method to apply, return mean over batch if 'mean',\n",
    "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
    "    Returns:\n",
    "        Loss tensor according to arg reduction\n",
    "    Raise:\n",
    "        Exception if unexpected reduction\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1, p=2, reduction='mean'):\n",
    "        super(BinaryDiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
    "        predict = predict.contiguous().view(predict.shape[0], -1)\n",
    "        target = target.contiguous().view(target.shape[0], -1)\n",
    "\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n",
    "        loss = 1 - num / den\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return loss\n",
    "        else:\n",
    "            raise Exception('Unexpected reduction {}'.format(self.reduction))\n",
    "        \n",
    "class CustomStopper(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='auto', start_epoch=5):\n",
    "        super().__init__(monitor=monitor, min_delta=min_delta, patience=patience, verbose=verbose, mode=mode)\n",
    "        self.start_epoch = start_epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"On epoch End!\")\n",
    "        if epoch > self.start_epoch:\n",
    "            super().on_epoch_end(epoch, logs)\n",
    "            print(\"On epoch End after starting point!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class DatasetForImputation:\n",
    "    def __init__(self, data, target, mask, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.target = target\n",
    "        self.mask = mask\n",
    "\n",
    "    def __len__(self):\n",
    "        length = len(self.target) - self.seq_len + 1\n",
    "        if length < 0:\n",
    "            return 0\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx : idx + self.seq_len, 1:]\n",
    "        target = self.target[idx : idx + self.seq_len]\n",
    "        mask = self.mask[idx : idx + self.seq_len]\n",
    "        return sample, target, mask\n",
    "\n",
    "class Imputation_helper:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def split_data(self, data, ratio):\n",
    "        split = int(ratio * len(data))\n",
    "        train = data[:split]\n",
    "        test = data[split:]\n",
    "        return train, test\n",
    "\n",
    "    def normalize_data(self, data, columns):\n",
    "        min_max = {}\n",
    "        mean_std = {}\n",
    "        for col in columns:\n",
    "            #min max normalization\n",
    "            # min_max[col] = [data[col].min(), data[col].max()]\n",
    "            # data[col] = (data[col] - data[col].min()) / (data[col].max() - data[col].min())\n",
    "\n",
    "            #standard normalization\n",
    "            mean_std[col] = [data[col].mean(), data[col].std()]\n",
    "            data[col] = (data[col] - data[col].mean()) / data[col].std()\n",
    "\n",
    "\n",
    "        return data, mean_std\n",
    "    \n",
    "    def reverse_normalize_data(self, data, columns, min_max):\n",
    "        for col in columns:\n",
    "            # data[col] = data[col] * (min_max[col][1] - min_max[col][0]) + min_max[col][0]\n",
    "            #reverse standard normalization\n",
    "            data[col] = data[col] * min_max[col][1] + min_max[col][0]\n",
    "        return data\n",
    "    \n",
    "    def get_data_loader(self, data, target, mask, shuffle, ratio = 0.8):\n",
    "        data = torch.tensor(data, dtype=torch.float32).to(self.config.device)\n",
    "        target = torch.tensor(target, dtype=torch.float32).to(self.config.device)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).to(self.config.device)\n",
    "        x_train, x_val = self.split_data(data, ratio)\n",
    "        y_train, y_val = self.split_data(target, ratio)\n",
    "        mask_train, mask_val = self.split_data(mask, ratio)\n",
    "        train_dataloader = DataLoader(DatasetForImputation(x_train, y_train, mask_train, self.config.lag_size), batch_size=self.config.batch_size, shuffle=shuffle, drop_last=True)\n",
    "        val_dataloader = DataLoader(DatasetForImputation(x_val, y_val, mask_val, self.config.lag_size), batch_size=self.config.batch_size, shuffle=shuffle, drop_last=True)\n",
    "        return train_dataloader, val_dataloader\n",
    "    \n",
    "    def convert_to_dataframe(self, data, columns):\n",
    "        data = pd.DataFrame(data, columns=columns)\n",
    "        return data\n",
    "    \n",
    "    def post_process_data(self, ground_truth, predicted, mask, col):\n",
    "        ground_truth[col] = (predicted[col] * mask[col]) + (ground_truth[col] * (1 - mask[col]))\n",
    "        return ground_truth\n",
    "    \n",
    "    def evaluation_metrics(self, ground_truth, predicted):\n",
    "        rmse = np.sqrt(mean_squared_error(ground_truth, predicted))\n",
    "        mae = np.mean(np.abs(ground_truth - predicted))\n",
    "        r2 = r2_score(ground_truth, predicted)\n",
    "\n",
    "        print(\"RMSE : \", rmse)\n",
    "        print(\"MAE : \", mae)\n",
    "        print(\"R2 : \", r2)\n",
    "       \n",
    "    def replace_data_with_imputed_data(self, data, imputed_data, missing_indexes, col):\n",
    "        for i in missing_indexes:\n",
    "            if i not in imputed_data.index:\n",
    "                continue\n",
    "            val = imputed_data[imputed_data.index == i][col].values[0]\n",
    "            data.loc[i, col] = val\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class ModelEvaluation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_memory_stats(self, model, input_shape=(), batch_size=1):\n",
    "        \"\"\"Get basic memory statistics for a model\"\"\"\n",
    "        \n",
    "        # Parameter memory\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        param_memory_mb = total_params * 4 / (1024**2)  # 4 bytes per float32\n",
    "        \n",
    "        # Inference memory (approximate)\n",
    "        model.eval()\n",
    "        dummy_input = torch.randn(batch_size, *input_shape)\n",
    "        \n",
    "        # Measure peak memory during forward pass\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            dummy_input = dummy_input.cuda()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _ = model(dummy_input)\n",
    "            \n",
    "            inference_memory_mb = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "        else:\n",
    "            # For CPU, approximate activation memory\n",
    "            with torch.no_grad():\n",
    "                _ = model(dummy_input)\n",
    "            inference_memory_mb = param_memory_mb * 2  # Rough estimate\n",
    "        \n",
    "        return {\n",
    "            'parameters': total_params,\n",
    "            'param_memory_mb': param_memory_mb,\n",
    "            'inference_memory_mb': inference_memory_mb\n",
    "        }\n",
    "\n",
    "    def get_tf_memory_stats(self, model, input_shape=(1, 256, 1)):\n",
    "        \"\"\"\n",
    "        Estimate parameter and inference memory usage of a TensorFlow model with input shape\n",
    "        (batch_size, sequence_length, feature_dim), e.g., for time-series or sequential models.\n",
    "\n",
    "        Parameters:\n",
    "            model: tf.keras.Model\n",
    "            input_shape: tuple like (batch_size, sequence_length, feature_dim)\n",
    "\n",
    "        Returns:\n",
    "            dict: parameters, param_memory_mb, inference_memory_mb\n",
    "        \"\"\"\n",
    "        # Total parameters\n",
    "        \n",
    "        total_params = model.count_params()\n",
    "        param_memory_mb = total_params * 4 / (1024 ** 2)  # float32 = 4 bytes\n",
    "\n",
    "        # Inference memory measurement\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            tf.keras.backend.clear_session()\n",
    "            tf.config.experimental.reset_memory_stats('GPU:0')\n",
    "\n",
    "            dummy_input = tf.random.normal(input_shape)\n",
    "            _ = model(dummy_input, training=False)\n",
    "\n",
    "            mem_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "            inference_memory_mb = mem_info['peak'] / (1024 ** 2)  # convert bytes to MB\n",
    "        else:\n",
    "            # Estimate for CPU (approximate only)\n",
    "            dummy_input = tf.random.normal(input_shape)\n",
    "            _ = model(dummy_input, training=False)\n",
    "            inference_memory_mb = param_memory_mb * 2  # conservative estimate\n",
    "\n",
    "        return {\n",
    "            'parameters': total_params,\n",
    "            'param_memory_mb': round(param_memory_mb, 2),\n",
    "            'inference_memory_mb': round(inference_memory_mb, 2)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
